n_views: &n_views 2
in_channels: &in_channels 3
seq_len: &seq_len 8
rm_global_scale: &rm_global_scale On
batch_size: 32
workers: 4
total_epochs: 100 
total_epochs_incremental_task: 100
step_per_epoch: &step_per_epoch On
step_per_batch: &step_per_batch Off
log_freq:
    train: 20
    val: null

model:
    name: 'dg_sta'
    in_channels: *in_channels
    n_heads: 8 # number of attention heads
    d_head: 32 # dimension of attention heads
    d_feat: 256 # feature dimension 128
    seq_len: *seq_len # sequence length
    n_joints: 50 # number of joints
    dropout: 0.5
    d_head_contrastive: 32


transforms:
    # window_size: *seq_len
    # p_interval: [0.5, 1]
    train:
        stratified_sample:
            n_samples: *seq_len
        # p_interval: [0.5, 1]
               
    val: &transforms_inference
        stratified_sample:
            n_samples: *seq_len
        # p_interval: [0.5, 1]
            
    testval: *transforms_inference  
    testtrain: *transforms_inference 
    test: *transforms_inference  




# loss:
#     contrastive: 
#         weight: 1.0
#         n_views: *n_views
#         is_supervised: On
#         temperature: 0.07
loss:
    cross_entropy: 
        weight: 1.0
        ignore: null
        class_weight: null


optimizer:
    name: 'adam'
    lr: &lr 0.0001
    lr_incremental_task: 0.00002
    betas: [0.9, 0.999]
    include_scheduler: False
    scheduler:
        name: 'linear'
        lr: *lr
        start_factor: 1
        end_factor: 0.001
        step_per_batch: *step_per_batch
        step_per_epoch: *step_per_epoch      
          

increm:
    load_pretrained_task0: False
    first_split_size: 60
    other_split_size: 1
    max_task: 7
    load_best_checkpoint_train: True  # Only for task_id == 1
    load_best_checkpoint_test: True # Only for task_id == 1
    freeze_feature_extractor: False
    freeze_classifier: False
    memory: 0
    learner:
        type: 'base'

