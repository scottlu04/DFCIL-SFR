{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3, 1, 4, 6, 4, 4, 4, 5, 3, 2, 4, 1, 2, 2, 5])\n",
      "torch.Size([16, 2, 16])\n",
      "tensor([5, 3, 1, 4, 6, 4, 4, 4, 5, 3, 2, 4, 1, 2, 2, 5, 5, 3, 1, 4, 6, 4, 4, 4,\n",
      "        5, 3, 2, 4, 1, 2, 2, 5])\n",
      "1\n",
      "tensor(2.8503, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "# from ret_benchmark.losses.registry import LOSS\n",
    "# from ret_benchmark.utils.log_info import log_info\n",
    "\n",
    "\n",
    "# @LOSS.register(\"contrastive_loss\")\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.margin = 0.5\n",
    "\n",
    "    def forward(self, inputs_col, targets_col):\n",
    "        inputs_col = torch.cat(torch.unbind(inputs_col, dim=1), dim=0)\n",
    "        targets_col = torch.cat([targets_col,targets_col])\n",
    "        print(targets_col)\n",
    "        inputs_row, target_row = inputs_col, targets_col\n",
    "        n = inputs_col.size(0)\n",
    "        # Compute similarity matrix\n",
    "        sim_mat = torch.matmul(inputs_col, inputs_row.t())\n",
    "        epsilon = 1e-5\n",
    "        loss = list()\n",
    "\n",
    "        neg_count = list()\n",
    "        for i in range(n):\n",
    "            pos_pair_ = torch.masked_select(sim_mat[i], targets_col[i] == target_row)\n",
    "            pos_pair_ = torch.masked_select(pos_pair_, pos_pair_ < 1 - epsilon)\n",
    "            neg_pair_ = torch.masked_select(sim_mat[i], targets_col[i] != target_row)\n",
    "\n",
    "            neg_pair = torch.masked_select(neg_pair_, neg_pair_ > self.margin)\n",
    "\n",
    "            pos_loss = torch.sum(-pos_pair_ + 1)\n",
    "            if len(neg_pair) > 0:\n",
    "                neg_loss = torch.sum(neg_pair)\n",
    "                neg_count.append(len(neg_pair))\n",
    "            else:\n",
    "                neg_loss = 0\n",
    "\n",
    "            loss.append(pos_loss + neg_loss)\n",
    "        print(1)\n",
    "        # if inputs_col.shape[0] == inputs_row.shape[0]:\n",
    "        #     prefix = \"batch_\"\n",
    "        # else:\n",
    "        #     prefix = \"memory_\"\n",
    "        # # if len(neg_count) != 0:\n",
    "        #     log_info[f\"{prefix}average_neg\"] = sum(neg_count) / len(neg_count)\n",
    "        # else:\n",
    "        #     log_info[f\"{prefix}average_neg\"] = 0\n",
    "        # log_info[f\"{prefix}non_zero\"] = len(neg_count)\n",
    "        loss = sum(loss) / n  # / all_targets.shape[1]\n",
    "        return loss\n",
    "\n",
    "def gen_data(bs, nv, d, c) :\n",
    "    xs = torch.rand(bs, nv, d)\n",
    "    ys = torch.randint(0, c, (bs, ))      \n",
    "    xs.requires_grad_(True)\n",
    "    return xs, ys\n",
    "\n",
    "bs, nv, d, c = 16, 2, 16, 10\n",
    "logits, label = gen_data(bs, nv, d, c)\n",
    "# logits = torch.squeeze(logits)\n",
    "print(label)\n",
    "print(logits.shape)\n",
    "loss = Loss()(logits, label)\n",
    "loss.backward()\n",
    "print(loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 2, 1, 0, 3, 4, 0, 8, 0, 7, 8, 5, 4, 3, 8, 5])\n",
      "torch.Size([16, 2, 16])\n",
      "(tensor([[0.3824, 0.0655, 0.5452, 0.0421, 0.6513, 0.5132, 0.5871, 0.4720, 0.1851,\n",
      "         0.4916, 0.2744, 0.1547, 0.6615, 0.8026, 0.1195, 0.6112],\n",
      "        [0.4206, 0.8958, 0.6608, 0.9208, 0.1771, 0.2190, 0.0054, 0.1661, 0.2441,\n",
      "         0.8426, 0.8671, 0.8522, 0.4704, 0.4017, 0.8272, 0.5905],\n",
      "        [0.6965, 0.1619, 0.9814, 0.8449, 0.6499, 0.2081, 0.6850, 0.5897, 0.5989,\n",
      "         0.2801, 0.3973, 0.5984, 0.9005, 0.1255, 0.9620, 0.7048],\n",
      "        [0.0292, 0.8827, 0.7805, 0.9655, 0.5377, 0.6211, 0.8064, 0.9332, 0.6489,\n",
      "         0.8588, 0.5309, 0.4430, 0.4684, 0.8366, 0.6080, 0.1456],\n",
      "        [0.2274, 0.6891, 0.4976, 0.6588, 0.6310, 0.5824, 0.0983, 0.1417, 0.0042,\n",
      "         0.3324, 0.8202, 0.6912, 0.2123, 0.3061, 0.0443, 0.7090],\n",
      "        [0.1678, 0.6298, 0.1818, 0.6747, 0.4092, 0.0872, 0.8836, 0.1598, 0.7189,\n",
      "         0.7128, 0.5129, 0.8580, 0.2035, 0.6982, 0.7599, 0.8857],\n",
      "        [0.0356, 0.9228, 0.3994, 0.2780, 0.4773, 0.4972, 0.5392, 0.6369, 0.2557,\n",
      "         0.5626, 0.3568, 0.4010, 0.0266, 0.1484, 0.5796, 0.4425],\n",
      "        [0.7102, 0.2627, 0.8565, 0.6356, 0.3534, 0.9607, 0.1989, 0.1333, 0.0347,\n",
      "         0.2133, 0.6740, 0.2382, 0.7790, 0.6363, 0.8604, 0.6110],\n",
      "        [0.7978, 0.5498, 0.6716, 0.8928, 0.8353, 0.7342, 0.8106, 0.5346, 0.1985,\n",
      "         0.0465, 0.1609, 0.2248, 0.1937, 0.5758, 0.4991, 0.9282],\n",
      "        [0.9852, 0.4185, 0.0382, 0.3621, 0.4631, 0.6253, 0.1938, 0.6034, 0.6611,\n",
      "         0.6633, 0.6224, 0.8390, 0.8218, 0.5799, 0.0306, 0.8867],\n",
      "        [0.7758, 0.3937, 0.5662, 0.7459, 0.6592, 0.3009, 0.0221, 0.4178, 0.4698,\n",
      "         0.4036, 0.5716, 0.2310, 0.4709, 0.0154, 0.6544, 0.3616],\n",
      "        [0.1563, 0.2257, 0.1206, 0.9964, 0.9687, 0.9024, 0.3202, 0.5017, 0.7431,\n",
      "         0.4503, 0.5252, 0.9898, 0.1524, 0.5450, 0.9865, 0.8077],\n",
      "        [0.2167, 0.8909, 0.9167, 0.8031, 0.5935, 0.5406, 0.5844, 0.9549, 0.4691,\n",
      "         0.7396, 0.9844, 0.6429, 0.8971, 0.3247, 0.6016, 0.5087],\n",
      "        [0.7622, 0.4405, 0.7717, 0.9488, 0.7357, 0.8383, 0.5850, 0.8943, 0.3294,\n",
      "         0.8362, 0.2353, 0.1107, 0.2660, 0.3510, 0.0640, 0.2443],\n",
      "        [0.2426, 0.7587, 0.2696, 0.6225, 0.9387, 0.6525, 0.7397, 0.2033, 0.1197,\n",
      "         0.3150, 0.0312, 0.8671, 0.9425, 0.9577, 0.1198, 0.0427],\n",
      "        [0.6657, 0.0461, 0.4139, 0.0735, 0.3745, 0.7279, 0.2031, 0.4796, 0.8134,\n",
      "         0.5341, 0.1471, 0.8359, 0.0391, 0.6798, 0.3243, 0.4220]],\n",
      "       grad_fn=<UnbindBackward0>), tensor([[0.1106, 0.2614, 0.0030, 0.5717, 0.5699, 0.0460, 0.3801, 0.7859, 0.3563,\n",
      "         0.6098, 0.9041, 0.7387, 0.6310, 0.2916, 0.0200, 0.8217],\n",
      "        [0.9288, 0.4114, 0.3049, 0.9601, 0.6793, 0.4287, 0.4666, 0.9895, 0.0439,\n",
      "         0.9701, 0.2674, 0.7038, 0.9444, 0.6656, 0.6033, 0.5890],\n",
      "        [0.2932, 0.8293, 0.8567, 0.9250, 0.0049, 0.9613, 0.6818, 0.7856, 0.5677,\n",
      "         0.6342, 0.8014, 0.6256, 0.0477, 0.1669, 0.0693, 0.6041],\n",
      "        [0.3370, 0.1786, 0.7147, 0.4843, 0.2283, 0.9167, 0.7120, 0.4190, 0.3852,\n",
      "         0.6743, 0.6908, 0.8083, 0.1201, 0.0883, 0.8349, 0.3738],\n",
      "        [0.1287, 0.7998, 0.8992, 0.7693, 0.7373, 0.6909, 0.1190, 0.8562, 0.6409,\n",
      "         0.8520, 0.7582, 0.8890, 0.6583, 0.7738, 0.4929, 0.9216],\n",
      "        [0.1026, 0.0996, 0.7405, 0.2204, 0.9407, 0.2299, 0.2655, 0.4362, 0.2372,\n",
      "         0.4353, 0.4530, 0.2445, 0.4442, 0.8592, 0.1155, 0.6413],\n",
      "        [0.7175, 0.8636, 0.8459, 0.9080, 0.3834, 0.9344, 0.3278, 0.5033, 0.8422,\n",
      "         0.4406, 0.4166, 0.7200, 0.9410, 0.0050, 0.7745, 0.5422],\n",
      "        [0.8389, 0.3414, 0.0025, 0.7936, 0.2406, 0.8372, 0.5941, 0.9275, 0.9706,\n",
      "         0.1364, 0.1316, 0.7765, 0.1621, 0.5710, 0.2189, 0.1745],\n",
      "        [0.5963, 0.3207, 0.5868, 0.0551, 0.4997, 0.9337, 0.8329, 0.3961, 0.3348,\n",
      "         0.8523, 0.8433, 0.7138, 0.6316, 0.9701, 0.0952, 0.7144],\n",
      "        [0.8211, 0.3556, 0.4906, 0.9882, 0.3607, 0.7858, 0.9677, 0.2724, 0.2839,\n",
      "         0.4453, 0.6581, 0.2964, 0.3584, 0.3597, 0.3729, 0.8329],\n",
      "        [0.4007, 0.2505, 0.2013, 0.3765, 0.9823, 0.8346, 0.7028, 0.5671, 0.6671,\n",
      "         0.5948, 0.0890, 0.5571, 0.8627, 0.9967, 0.7948, 0.1416],\n",
      "        [0.2312, 0.0771, 0.1345, 0.2123, 0.8794, 0.2375, 0.8494, 0.9807, 0.1308,\n",
      "         0.1520, 0.6201, 0.1870, 0.3516, 0.5834, 0.1266, 0.0115],\n",
      "        [0.8351, 0.9186, 0.9809, 0.9306, 0.2975, 0.8084, 0.8189, 0.6154, 0.3946,\n",
      "         0.4862, 0.5553, 0.8909, 0.3998, 0.6195, 0.7891, 0.9329],\n",
      "        [0.1572, 0.6258, 0.4043, 0.0293, 0.6126, 0.2130, 0.1684, 0.6616, 0.0374,\n",
      "         0.6688, 0.5585, 0.2243, 0.4144, 0.1606, 0.6406, 0.0255],\n",
      "        [0.1141, 0.8184, 0.0625, 0.8710, 0.7156, 0.7587, 0.6861, 0.6726, 0.9193,\n",
      "         0.8763, 0.2173, 0.8029, 0.4573, 0.5623, 0.0653, 0.5340],\n",
      "        [0.6356, 0.2388, 0.9428, 0.8494, 0.1170, 0.5193, 0.4451, 0.2473, 0.6175,\n",
      "         0.7708, 0.8168, 0.1138, 0.6347, 0.0919, 0.6756, 0.5845]],\n",
      "       grad_fn=<UnbindBackward0>))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(label)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)    \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mLoss.forward\u001b[0;34m(self, inputs_col, targets_col)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs_col)\n\u001b[1;32m     18\u001b[0m inputs_row, target_row \u001b[38;5;241m=\u001b[39m inputs_col, targets_col\n\u001b[0;32m---> 19\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43minputs_col\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute similarity matrix\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sim_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(inputs_col, inputs_row\u001b[38;5;241m.\u001b[39mt())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
